{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30966ca7",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "In this exercise, you will use the Custom Vision service to train an object detection model that can detect and locate three classes of fruit (apple, banana, and orange) in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36760582",
   "metadata": {},
   "source": [
    "### Prerequisite\n",
    "\n",
    "- Create an Azure Custom Vision Resource\n",
    "    - (technically, you are creating two resources. One for training and one for prediction)\n",
    "- Create a Custom Vision project in the Custom Vision portal\n",
    "    - You could do this in code via the SDK: [Link Here](https://learn.microsoft.com/en-us/azure/ai-services/custom-vision-service/quickstarts/object-detection?tabs=windows%2Cvisual-studio&pivots=programming-language-python)\n",
    "- Upload and tag images (supervised learning)\n",
    "    - Similarly, you could also do this via code (refer to the link above)\n",
    "    - After the images have been uploaded, select the first one to open it.\n",
    "    - Hold the mouse over any object in the image until an automatically detected region is displayed\n",
    "    - When the region surrounds the object, add a new tag with the appropriate object type (apple, banana, or orange)\n",
    "\n",
    "- Train the model (or wait until you upload files using the SDK)\n",
    "    - Instructor note: I did most of the prework, leaving one image to be tagged in the Studio\n",
    "\n",
    "- Look at the model's performance and test the model with some images you find online\n",
    "\n",
    "- Create your `.env` file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51693022",
   "metadata": {},
   "source": [
    "### Use the Custom Vision SDK to upload images\n",
    "\n",
    "You can use the UI in the Custom Vision portal to tag your images, but many AI development teams use other tools that generate files containing information about tags and object regions in images. In scenarios like this, you can use the Custom Vision training API to upload tagged images to the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240ee89",
   "metadata": {},
   "source": [
    "#### You Will Need\n",
    "- the Project ID (from the computer vision settings in the portal. you could fetch this using the SDK as well)\n",
    "- custom vision Endpoint and Key (for training service)\n",
    "\n",
    "Then refer to the 04b. Add_tagged_images.ipynb file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65753f",
   "metadata": {},
   "source": [
    "### Bounding Box Field Definitions\n",
    "\n",
    "---\n",
    "\n",
    "**1. `left`**  \n",
    "This is the **normalized horizontal coordinate** of the top-left corner of the bounding box.  \n",
    "- Measured as a fraction of the total image width.  \n",
    "- Example: `left = 0.5` means the box starts halfway across the image horizontally.  \n",
    "- So `left = 0.117563143` means about 11.75% of the way across the image from the left edge.\n",
    "\n",
    "---\n",
    "\n",
    "**2. `top`**  \n",
    "This is the **normalized vertical coordinate** of the top-left corner of the bounding box.  \n",
    "- Measured as a fraction of the total image height.  \n",
    "- Example: `top = 0.141307935` means the box starts about 14.13% down from the top edge.\n",
    "\n",
    "---\n",
    "\n",
    "**3. `width`**  \n",
    "This is the **normalized width** of the bounding box.  \n",
    "- Expressed as a fraction of the imageâ€™s total width.  \n",
    "- Example: `width = 0.3` means the box spans 30% of the total image width.\n",
    "\n",
    "---\n",
    "\n",
    "**4. `height`**  \n",
    "This is the **normalized height** of the bounding box.  \n",
    "- Expressed as a fraction of the imageâ€™s total height.  \n",
    "- Example: `height = 0.421263933` means the box is about 42.13% of the total image height.\n",
    "\n",
    "---\n",
    "\n",
    "### Putting It Together\n",
    "\n",
    "If your image is **1000 pixels wide** and **800 pixels tall**, then for the `\"orange\"` box:\n",
    "\n",
    "- `left = 0.501782656` â†’ pixel **501.78** (of 1000) from the left  \n",
    "- `top = 0.141307935` â†’ pixel **113.04** (of 800) from the top  \n",
    "- `width = 0.30014348` â†’ box width **300.14 pixels**  \n",
    "- `height = 0.421263933` â†’ box height **337.01 pixels**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f9f46",
   "metadata": {},
   "source": [
    "### Test A Model\n",
    "\n",
    "- The model will need to be trained first. Do so with the train option in the studio\n",
    "- In the Custom Vision portal, when training has finished, review the Precision, Recall, and mAP performance metrics - these measure the prediction accuracy of the object detection model, and should all be high.\n",
    "- At the top right of the page, click Quick Test, and then in the Image URL box, type `https://aka.ms/test-fruit` and click the quick test image (âž”) button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb44659",
   "metadata": {},
   "source": [
    "#### Using the SDK to use the object detector in a client application\n",
    "\n",
    "Publish the object detection model\n",
    "\n",
    "\n",
    "- In the Custom Vision portal, on the Performance page, click ðŸ—¸ Publish to publish the trained model with the following settings:\n",
    "    - Model name: fruit-detector\n",
    "    - Prediction Resource: The prediction resource you created previously which ends with â€œ-Predictionâ€ (not the training resource).\n",
    "- make sure to note the prediction service Endpoint and key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed36cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
    "from msrest.authentication import ApiKeyCredentials\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6f1d357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda71bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_endpoint = os.getenv('CUSTOM_IMAGE_PREDICT_ENDPOINT')\n",
    "prediction_key = os.getenv('CUSTOM_IMAGE_PREDICT_KEY')\n",
    "project_id = \"97462dc8-2998-4044-951f-ed14d6ad89d9\"\n",
    "model_name = os.getenv('ModelName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041ffd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": prediction_key})\n",
    "prediction_client = CustomVisionPredictionClient(endpoint=prediction_endpoint, credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a06693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect objects in the image\n",
    "image_file = 'images/Object Detection/test_images/produce.jpg'\n",
    "#print('Detecting objects in', image_file)\n",
    "with open(image_file, mode=\"rb\") as image_data:\n",
    "    results = prediction_client.detect_image(project_id, model_name, image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10f627",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f05d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each prediction\n",
    "for prediction in results.predictions:\n",
    "    # Get each prediction with a probability > 50%\n",
    "    if (prediction.probability*100) > 50:\n",
    "        print(prediction.tag_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62bfd7",
   "metadata": {},
   "source": [
    "#### We could even annotate our Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea57280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tagged_image(source_path, detected_objects):\n",
    "    #Load the image using Pillow\n",
    "    image = Image.open(source_path)\n",
    "    h, w, ch = np.array(image).shape\n",
    "    # Create a figure for the results\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Display the image with boxes around each detected object\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    lineWidth = int(w/100)\n",
    "    color = 'magenta'\n",
    "    for detected_object in detected_objects:\n",
    "        # Only show objects with a > 50% probability\n",
    "        if (detected_object.probability*100) > 50:\n",
    "            # Box coordinates and dimensions are proportional - convert to absolutes\n",
    "            left = detected_object.bounding_box.left * w \n",
    "            top = detected_object.bounding_box.top * h \n",
    "            height = detected_object.bounding_box.height * h\n",
    "            width =  detected_object.bounding_box.width * w\n",
    "            # Draw the box\n",
    "            points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))\n",
    "            draw.line(points, fill=color, width=lineWidth)\n",
    "            # Add the tag name and probability\n",
    "            plt.annotate(detected_object.tag_name + \": {0:.2f}%\".format(detected_object.probability * 100),(left,top), backgroundcolor=color)\n",
    "    plt.imshow(image)\n",
    "    outputfile = 'images/Object Detection/test_images/annotated_test_image.jpg'\n",
    "    fig.savefig(outputfile)\n",
    "    print('Results saved in', outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save an annotated image\n",
    "save_tagged_image(image_file, results.predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
