{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18fe5560",
   "metadata": {},
   "source": [
    "The Speech service provides speech to text and text to speech capabilities with a Speech resource. <br/>You can transcribe speech to text with high accuracy, produce natural-sounding text to speech voices, translate spoken audio, and use speaker recognition during conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad2904",
   "metadata": {},
   "source": [
    "In this Demo, we will show off the speech to text capabilities <br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a63d27",
   "metadata": {},
   "source": [
    "The speech to text service offers the following core features:\n",
    "\n",
    "- Real-time transcription: Instant transcription with intermediate results for live audio inputs.\n",
    "- Fast transcription: Fastest synchronous output for situations with predictable latency.\n",
    "- Batch transcription: Efficient processing for large volumes of prerecorded audio.\n",
    "- Custom speech: Models with enhanced accuracy for specific domains and conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce02d6f9",
   "metadata": {},
   "source": [
    "### Batch Transcription\n",
    "You should provide multiple files per request or point to an Azure Blob Storage container with the audio files to transcribe. The batch transcription service can handle a large number of submitted transcriptions. The service transcribes the files concurrently, which reduces the turnaround time. <br/>\n",
    "**Batch transcription should only be done with the REST API, not he SDK**\n",
    "\n",
    "##### How does it work?\n",
    "With batch transcriptions, you submit the audio data, and then retrieve transcription results asynchronously. The service transcribes the audio data and stores the results in a storage container. You can then retrieve the results from the storage container.\n",
    "\n",
    "(Batch transcription jobs are scheduled on a best-effort basis. At peak hours it might take up to 30 minutes or longer for a transcription job to start processing. )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a0a9b7",
   "metadata": {},
   "source": [
    "### Fast Transcription\n",
    "Fast transcription API is used to transcribe audio files with returning results synchronously and faster than real-time audio. Use fast transcription in the scenarios that you need the transcript of an audio recording as quickly as possible with predictable latency, such as:\n",
    "\n",
    "Quick audio or video transcription and subtitles: Quickly get a transcription of an entire video or audio file in one go.\n",
    "Video translation: Immediately get new subtitles for a video if you have audio in different languages.\n",
    "\n",
    "**Only available via the API**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cad7d0c",
   "metadata": {},
   "source": [
    "### Real-Time Transcription\n",
    "Real-time speech to text transcribes audio as it's recognized from a microphone or file. <br/>\n",
    "Available via the SDK and REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f0ae8",
   "metadata": {},
   "source": [
    "##### From a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae4c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-cognitiveservices-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a92492f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84224de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaec73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = os.getenv('AI_SPEECH_ENDPOINT')\n",
    "key = os.getenv('AI_SPEECH_KEY')\n",
    "region = 'eastus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aside: a lot of the time, we will have this scripts as .py files, so you will write functions\n",
    "def transcribe(filepath):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=key,\n",
    "                                           #endpoint=base_url,\n",
    "                                           region=region)\n",
    "    speech_config.speech_recognition_language = 'en-US'\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=filepath)\n",
    "\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(f\"Recognized: {speech_recognition_result.text}\")\n",
    "        return speech_recognition_result.text\n",
    "\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(f\"No speech could be recognized: {speech_recognition_result.no_match_details}\")\n",
    "    \n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(f\"Speech Recognition canceled: {cancellation_details.reason}\")\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35087250",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe('./Data/FightMilk.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409026af",
   "metadata": {},
   "source": [
    "##### From a Microphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_from_microphone():\n",
    "     # Replace with your own subscription key and endpoint, the endpoint is like : \"https://YourServiceRegion.api.cognitive.microsoft.com\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=key, endpoint=base_url)\n",
    "    speech_config.speech_recognition_language=\"en-US\"\n",
    "\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True) #you can also use the device if you need to specify a microphone\n",
    "    #https://learn.microsoft.com/en-us/azure/ai-services/speech-service/how-to-select-audio-input-devices\n",
    "    \n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "    #speech_recognition_result = speech_recognizer.recognize_once()\n",
    "    #both of these will effectively do the same thing here\n",
    "\n",
    "    #this one will not output the text by default, but you can print the result\n",
    "\n",
    "\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and endpoint values?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efcba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recognize_from_microphone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93afa86",
   "metadata": {},
   "source": [
    "**NOTE: This example uses the recognize_once_async operation to transcribe utterances of up to 30 seconds, or until silence is detected.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa30eb2",
   "metadata": {},
   "source": [
    "#### For continous recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d527d",
   "metadata": {},
   "source": [
    "The previous examples use single-shot recognition, which recognizes a single utterance. The end of a single utterance is determined by listening for silence at the end or until a maximum of 15 (or 30?) seconds of audio is processed. <br/><br/>\n",
    "In contrast, you use continuous recognition when you want to control when to stop recognizing. It requires you to connect to EventSignal to get the recognition results. <br/>To stop recognition, you must call `stop_continuous_recognition()` or `stop_continuous_recognition_async()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08f8652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecc3e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_recognition(filepath = None):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=key, endpoint=base_url)\n",
    "    speech_config.speech_recognition_language = 'en-US'\n",
    "\n",
    "    if filepath:\n",
    "        audio_config = speechsdk.audio.AudioConfig(filename=filepath)\n",
    "    else:\n",
    "        audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    #Variable to manage state\n",
    "    done = False\n",
    "\n",
    "    #create a callback to stop continuous recognition when evt is received.\n",
    "        # When evt is received, the evt message is printed.\n",
    "        # After evt is received, stop_continuous_recognition() is called to stop recognition.\n",
    "        # The recognition state is changed to True.\n",
    "    \n",
    "    def stop_cb(evt):\n",
    "        '''\n",
    "        This function is triggered when certain events fire (you connect it later).\n",
    "        '''\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "        #global done if doing this in a py file\n",
    "        # In a .py script outside Jupyter, you’d need global done inside stop_cb to modify the done variable defined outside its scope.\n",
    "        # if you still get issues, do nonlocal done\n",
    "        done = True\n",
    "    \n",
    "    # Connecting event handlers\n",
    "    '''\n",
    "    - recognizing: fires while the recognizer is receiving audio and producing interim results.\n",
    "    - recognized: fires when final text is recognized (end of an utterance or pause).\n",
    "    - session_started: signals the speech service session has started. Good for logging or UI readiness.\n",
    "    - session_stopped: signals the session has ended normally.\n",
    "        - You connect stop_cb to this below to shut down cleanly.\n",
    "    - canceled: fires if the recognition is canceled, e.g., by error or manual stop. Also triggers stop_cb to clean up.\n",
    "    '''\n",
    "    speech_recognizer.recognizing.connect(lambda evt: print('RECOGNIZING: {}'.format(evt)))\n",
    "    #To just get the text, replace above with : speech_recognizer.recognizing.connect( lambda evt: print('RECOGNIZING: {}'.format(evt.result.text)))\n",
    "    #or comment out recognizing, and olnly get the final output\n",
    "    speech_recognizer.recognized.connect(lambda evt: print('RECOGNIZED: {}'.format(evt)))\n",
    "    #To just get the text, replace above with: speech_recognizer.recognized.connect(lambda evt: print('RECOGNIZED: {}'.format(evt.result.text)))\n",
    "\n",
    "    speech_recognizer.session_started.connect(lambda evt: print('SESSION STARTED: {}'.format(evt)))\n",
    "    speech_recognizer.session_stopped.connect(lambda evt: print('SESSION STOPPED {}'.format(evt)))\n",
    "    speech_recognizer.canceled.connect(lambda evt: print('CANCELED {}'.format(evt)))\n",
    "\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    #Now that everything is setup, we can call the start_continuous_recognition method to start the recognition process.\n",
    "    \n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        '''\n",
    "        - Keeps your Python script or notebook alive so the recognizer thread keeps running.\n",
    "        - The loop polls the done flag every 0.5 seconds.\n",
    "        - Once stop_cb sets done = True, this loop exits and your function ends.\n",
    "        '''\n",
    "        time.sleep(.5) \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ca2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_recognition('./Data/FightMilk.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b1c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_recognition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475ec7a6",
   "metadata": {},
   "source": [
    "### Stop if there has been 3 seconds of silence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82fa8f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065143d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_recognition_timed(filepath = None, silence_timeout_sec=3):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=key, endpoint=base_url)\n",
    "    speech_config.speech_recognition_language = 'en-US'\n",
    "\n",
    "    if filepath:\n",
    "        audio_config = speechsdk.audio.AudioConfig(filename=filepath)\n",
    "    else:\n",
    "        audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    #Variable to manage state\n",
    "    done = False\n",
    "    silence_deadline = time.time() + silence_timeout_sec #SET THE DEADLINE\n",
    "    \n",
    "    def stop_cb(evt):\n",
    "        nonlocal done\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "        done = True\n",
    "\n",
    "    def recognizing_cb(evt):\n",
    "        nonlocal silence_deadline\n",
    "        print('RECOGNIZING: {}'.format(evt))\n",
    "        silence_deadline = time.time() + silence_timeout_sec\n",
    "    \n",
    "    def recognized_cb(evt): #refactoring this to calculate new dealine each time\n",
    "        nonlocal silence_deadline\n",
    "        print('RECOGNIZED: {}'.format(evt))\n",
    "        silence_deadline = time.time() + silence_timeout_sec\n",
    "\n",
    "\n",
    "    speech_recognizer.recognizing.connect(recognizing_cb)\n",
    "    #speech_recognizer.recognized.connect(lambda evt: print('RECOGNIZED: {}'.format(evt)))\n",
    "    speech_recognizer.recognized.connect(recognized_cb)\n",
    "    speech_recognizer.session_started.connect(lambda evt: print('SESSION STARTED: {}'.format(evt)))\n",
    "    speech_recognizer.session_stopped.connect(lambda evt: print('SESSION STOPPED {}'.format(evt)))\n",
    "    speech_recognizer.canceled.connect(lambda evt: print('CANCELED {}'.format(evt)))\n",
    "\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    #Now that everything is setup, we can call the start_continuous_recognition method to start the recognition process.\n",
    "    \n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        time.sleep(.5)\n",
    "        if time.time() > silence_deadline:\n",
    "            print(f\"No speech for {silence_timeout_sec} seconds. Stopping recognition.\")\n",
    "            speech_recognizer.stop_continuous_recognition()\n",
    "            done = True\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e27b97d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SESSION STARTED: SessionEventArgs(session_id=e49b9129357f4924bf1d2d53d6dd43eb)\n",
      "RECOGNIZING: SpeechRecognitionEventArgs(session_id=e49b9129357f4924bf1d2d53d6dd43eb, result=SpeechRecognitionResult(result_id=430d862181724946803b41b229f461ec, text=\"CS go\", reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZING: SpeechRecognitionEventArgs(session_id=e49b9129357f4924bf1d2d53d6dd43eb, result=SpeechRecognitionResult(result_id=e213302b8d584fb5a9b595990dade669, text=\"CS go i'm not sure\", reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZING: SpeechRecognitionEventArgs(session_id=e49b9129357f4924bf1d2d53d6dd43eb, result=SpeechRecognitionResult(result_id=a1cf73e1ab3d4c7a86d0974e7c996aab, text=\"CS go i'm not sure if that\", reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZING: SpeechRecognitionEventArgs(session_id=e49b9129357f4924bf1d2d53d6dd43eb, result=SpeechRecognitionResult(result_id=a1d4e66bf094480fab2a072351d60c10, text=\"CS go i'm not sure if\", reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZING: SpeechRecognitionEventArgs(session_id=e49b9129357f4924bf1d2d53d6dd43eb, result=SpeechRecognitionResult(result_id=2abd9688069c4c64abe58774c569e55e, text=\"CS go i'm not sure if that's really\", reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZING: SpeechRecognitionEventArgs(session_id=e49b9129357f4924bf1d2d53d6dd43eb, result=SpeechRecognitionResult(result_id=b7fc40f2a84b47f28638aedd76235b23, text=\"CS go i'm not sure if that's really going to be\", reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZING: SpeechRecognitionEventArgs(session_id=e49b9129357f4924bf1d2d53d6dd43eb, result=SpeechRecognitionResult(result_id=8ad5af7c8cd341db825227c6193256d3, text=\"CS go i'm not sure if that's really going to be the issue\", reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZING: SpeechRecognitionEventArgs(session_id=e49b9129357f4924bf1d2d53d6dd43eb, result=SpeechRecognitionResult(result_id=92f477caf4cb4702b200a59ade4ab840, text=\"CS go i'm not sure if that's really going to be the issue but we'll see\", reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZED: SpeechRecognitionEventArgs(session_id=e49b9129357f4924bf1d2d53d6dd43eb, result=SpeechRecognitionResult(result_id=ad24f15ce1134d9083100fcf85552df7, text=\"CS GO. I'm not sure if that's really going to be the issue, but we'll see.\", reason=ResultReason.RecognizedSpeech))\n",
      "RECOGNIZING: SpeechRecognitionEventArgs(session_id=e49b9129357f4924bf1d2d53d6dd43eb, result=SpeechRecognitionResult(result_id=273f3e94ed7940f49cb3473b9e196f3f, text=\"OK\", reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZED: SpeechRecognitionEventArgs(session_id=e49b9129357f4924bf1d2d53d6dd43eb, result=SpeechRecognitionResult(result_id=aad02bf8fb714f11b5ac6c5f5f5083fb, text=\"OK, wow.\", reason=ResultReason.RecognizedSpeech))\n",
      "No speech for 3 seconds. Stopping recognition.\n",
      "SESSION STOPPED SessionEventArgs(session_id=e49b9129357f4924bf1d2d53d6dd43eb)\n",
      "CLOSING on SessionEventArgs(session_id=e49b9129357f4924bf1d2d53d6dd43eb)\n"
     ]
    }
   ],
   "source": [
    "continuous_recognition_timed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0344ab",
   "metadata": {},
   "source": [
    "### In one Language, Out the other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f935b0a1",
   "metadata": {},
   "source": [
    "##### What our function needs to do\n",
    "- Listen to the microphone continuously.\n",
    "- Recognize speech in the source language.\n",
    "- Feed the recognized text to Azure’s TTS.\n",
    "- *Speak it back out loud in the target language.\n",
    "    - This function will save the speech in a file with the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d90c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def echo_with_tts(target_language='fr-FR'):\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=key, endpoint=base_url)\n",
    "    speech_config.speech_recognition_language = 'en-US'\n",
    "\n",
    "    audio_config_in = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config_in)\n",
    "\n",
    "    # Synthesis config: pick target voice matching the target language\n",
    "    speech_config.speech_synthesis_language = target_language\n",
    "\n",
    "    # Pick a voice name. You can look up the list in Azure docs.\n",
    "    # For French:\n",
    "    speech_config.speech_synthesis_voice_name = 'fr-FR-DeniseNeural'\n",
    "    #you could set up some logic here to choose the voice based on the target language\n",
    "\n",
    "    audio_config_out = speechsdk.audio.AudioConfig(filename=\"output_test.wav\")\n",
    "    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config_out)\n",
    "    # IF you want to do this real time, use a .py file and set audio_config_out to None\n",
    "\n",
    "    done = False\n",
    "\n",
    "    def recognized_cb(evt):\n",
    "        text = evt.result.text\n",
    "        print(f\"RECOGNIZED: {text}\")\n",
    "\n",
    "        # Now speak it in the target language\n",
    "        if text.strip():\n",
    "            print(f\"Speaking in {target_language}...\")\n",
    "            result = speech_synthesizer.speak_text_async(text).get()\n",
    "\n",
    "            if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "                print(\"Speech synthesized for text [{}]\".format(text))\n",
    "            elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "                cancellation_details = result.cancellation_details\n",
    "                print(\"Speech synthesis canceled: {}\".format(cancellation_details.reason))\n",
    "                if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "                    print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "\n",
    "    def stop_cb(evt):\n",
    "        nonlocal done\n",
    "        print(f\"Session ended: {evt}\")\n",
    "        speech_recognizer.stop_continuous_recognition()\n",
    "        done = True\n",
    "\n",
    "    speech_recognizer.recognized.connect(recognized_cb)\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "\n",
    "    while not done:\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2ba82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECOGNIZED: All right, we're going to see if this works this time.\n",
      "Speaking in fr-FR...\n",
      "RECOGNIZED: All right, we're going to see if this works this time.\n",
      "Speaking in fr-FR...\n",
      "Speech synthesized for text [All right, we're going to see if this works this time.]\n",
      "Speech synthesized for text [All right, we're going to see if this works this time.]\n",
      "RECOGNIZED: Interesting to see what happens.\n",
      "Speaking in fr-FR...\n",
      "RECOGNIZED: Interesting to see what happens.\n",
      "Speaking in fr-FR...\n",
      "Speech synthesized for text [Interesting to see what happens.]\n",
      "Speech synthesized for text [Interesting to see what happens.]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mecho_with_tts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mecho_with_tts\u001b[39m\u001b[34m(target_language)\u001b[39m\n\u001b[32m     49\u001b[39m speech_recognizer.start_continuous_recognition()\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECOGNIZED: \n",
      "RECOGNIZED: \n",
      "RECOGNIZED: No way.\n",
      "Speaking in fr-FR...\n",
      "RECOGNIZED: No way.\n",
      "Speaking in fr-FR...\n",
      "Speech synthesized for text [No way.]\n",
      "Speech synthesized for text [No way.]\n",
      "RECOGNIZED: No way.\n",
      "Speaking in fr-FR...\n",
      "RECOGNIZED: No way.\n",
      "Speaking in fr-FR...\n",
      "Speech synthesized for text [No way.]\n",
      "Speech synthesized for text [No way.]\n",
      "RECOGNIZED: \n",
      "RECOGNIZED: \n",
      "RECOGNIZED: \n",
      "RECOGNIZED: \n",
      "RECOGNIZED: \n",
      "RECOGNIZED: \n",
      "RECOGNIZED: \n"
     ]
    }
   ],
   "source": [
    "echo_with_tts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e045d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
